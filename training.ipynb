{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from feature_utils import TensorMaker\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys(data):\n",
    "    if isinstance(data, dict):\n",
    "        if 'data' in data.keys():\n",
    "            temp = {}\n",
    "            for k, v in data['data'].items(): temp[int(k)] = v\n",
    "            data['data'] = temp\n",
    "            return data\n",
    "        else:\n",
    "            return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# load CoNLL2003\n",
    "train = json.load(open('data/conll2003/en/train.json'), object_hook = convert_keys)\n",
    "valid = json.load(open('data/conll2003/en/valid.json'), object_hook = convert_keys)\n",
    "test = json.load(open('data/conll2003/en/test.json'), object_hook = convert_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabularies...\n",
      "Initializing TensorMaker...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vocabularies...\")\n",
    "\n",
    "WORDS, TAGS = list(), list()\n",
    "for _, d in itertools.chain(train['data'].items(), valid['data'].items(), test['data'].items()):\n",
    "    for w in d['sentence']:\n",
    "        if w.lower() not in WORDS: WORDS.append(w.lower())\n",
    "            \n",
    "    for t in d['tags']:\n",
    "        if t not in TAGS: TAGS.append(t)\n",
    "        \n",
    "        \n",
    "##### initialize TensorMaker ######\n",
    "\n",
    "print(\"Initializing TensorMaker...\")\n",
    "\n",
    "MAX_LEN_SENT = 125   # maximum sentence length\n",
    "MAX_LEN_WORD = 20    # maximum word length\n",
    "\n",
    "TM = TensorMaker(WORDS, TAGS, max_len_word=MAX_LEN_WORD, word_padding='post', word_truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word embeddings...\")\n",
    "\n",
    "d = 50\n",
    "emb_dir = 'embeddings/glove.6B/glove.6B.{}d.txt'.format(d)\n",
    "\n",
    "e = open(emb_dir, encoding='UTF-8')\n",
    "\n",
    "embeddings = dict()\n",
    "for line in e:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coef = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coef\n",
    "    \n",
    "e.close()\n",
    "\n",
    "embedding_dim = (len(TM.word2idx), d)\n",
    "E = np.zeros(embedding_dim)\n",
    "\n",
    "for i, w in enumerate(TM.word2idx):\n",
    "    emb_vec = embeddings.get(w)\n",
    "    if emb_vec is not None:\n",
    "        E[i,:] = emb_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model...\n"
     ]
    }
   ],
   "source": [
    "WORD_EMBEDDING_LAYER = 8\n",
    "\n",
    "print(\"Defining model...\")\n",
    "\n",
    "from models.blstm_cnn_crf_word_features_model import blstm_cnn_wd_ft_ner\n",
    "from models.blstm_crf_model import blstm_ner\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "# model = blstm_ner(max_len_sent=MAX_LEN_SENT,\n",
    "#                   embedding_dims=embedding_dim,\n",
    "#                   num_tags=len(TM.tag2idx))\n",
    "\n",
    "model = blstm_cnn_wd_ft_ner(max_len_sent=MAX_LEN_SENT,\n",
    "                            max_len_word=MAX_LEN_WORD,\n",
    "                            num_tags=len(TM.tag2idx),\n",
    "                            word_embedding_dims=embedding_dim,\n",
    "                            char_embedding_dims=(len(TM.char2idx), 25),\n",
    "                            word_feature_embedding_dims=(6,10),\n",
    "                            main_dropout=0.25,\n",
    "                            char_dropout=0.25,\n",
    "                            recurrent_dropout=0.25)\n",
    "\n",
    "\n",
    "model.layers[WORD_EMBEDDING_LAYER].set_weights([E])\n",
    "model.layers[WORD_EMBEDDING_LAYER].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n",
    "from generators import DataGenerator\n",
    "\n",
    "\n",
    "from validation import sentence_metrics\n",
    "from generators import TestDataGenerator\n",
    "\n",
    "TG = TestDataGenerator(test['data'], BATCH_SIZE, TM, shuffle=True, sentence=True, characters=True, word_features=True)\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.005), loss=crf_loss, metrics=[crf_accuracy])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_crf_accuracy', min_delta=0.001, patience=9)\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor='val_crf_accuracy', \n",
    "                                      factor=0.2, \n",
    "                                      patience=5,\n",
    "                                      mode='max',\n",
    "                                      min_lr=0.0001)\n",
    "\n",
    "MODEL_NAME = time.time()\n",
    "checkpointer = ModelCheckpoint(filepath='./trained_models/model_{}'.format(MODEL_NAME), verbose=True, save_best_only=True)\n",
    "\n",
    "tb = TensorBoard(log_dir='./tf_logs/blstm_cnn_crf_{}'.format(time.time()),\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 write_grads=True,\n",
    "                 write_graph=False,\n",
    "                 histogram_freq=0)\n",
    "\n",
    "DG = DataGenerator(data=train['data'], batch_size=BATCH_SIZE, tensor_maker=TM, shuffle=True, sentences=True, characters=True, word_features=True, tags=True)\n",
    "VG = DataGenerator(data=valid['data'], batch_size=BATCH_SIZE, tensor_maker=TM, shuffle=True, sentences=True, characters=True, word_features=True, tags=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "476/476 [==============================] - 40s 85ms/step - loss: 0.0459 - crf_accuracy: 0.9809 - val_loss: 0.1209 - val_crf_accuracy: 0.9609\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12087, saving model to ./trained_models/model_1550375726.608014\n",
      "Epoch 2/75\n",
      "476/476 [==============================] - 40s 85ms/step - loss: 0.0491 - crf_accuracy: 0.9792 - val_loss: 0.1081 - val_crf_accuracy: 0.9643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12087 to 0.10807, saving model to ./trained_models/model_1550375726.608014\n",
      "Epoch 3/75\n",
      "476/476 [==============================] - 39s 83ms/step - loss: 0.0512 - crf_accuracy: 0.9788 - val_loss: 0.1109 - val_crf_accuracy: 0.9632\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10807\n",
      "Epoch 4/75\n",
      "476/476 [==============================] - 47s 99ms/step - loss: 0.0518 - crf_accuracy: 0.9789 - val_loss: 0.1146 - val_crf_accuracy: 0.9594\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10807\n",
      "Epoch 5/75\n",
      "476/476 [==============================] - 44s 93ms/step - loss: 0.0511 - crf_accuracy: 0.9788 - val_loss: 0.1137 - val_crf_accuracy: 0.9618\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10807\n",
      "Epoch 6/75\n",
      "476/476 [==============================] - 40s 83ms/step - loss: 0.0518 - crf_accuracy: 0.9785 - val_loss: 0.1193 - val_crf_accuracy: 0.9599\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10807\n",
      "Epoch 7/75\n",
      "476/476 [==============================] - 43s 90ms/step - loss: 0.0529 - crf_accuracy: 0.9783 - val_loss: 0.1003 - val_crf_accuracy: 0.9662\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10807 to 0.10032, saving model to ./trained_models/model_1550375726.608014\n",
      "Epoch 8/75\n",
      "476/476 [==============================] - 62s 129ms/step - loss: 0.0517 - crf_accuracy: 0.9788 - val_loss: 0.1064 - val_crf_accuracy: 0.9622\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10032\n",
      "Epoch 9/75\n",
      "476/476 [==============================] - 58s 122ms/step - loss: 0.0524 - crf_accuracy: 0.9789 - val_loss: 0.1066 - val_crf_accuracy: 0.9630\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10032\n",
      "Epoch 10/75\n",
      "303/476 [==================>...........] - ETA: 19s - loss: 0.0497 - crf_accuracy: 0.9795"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=DG,\n",
    "                    validation_data=VG,\n",
    "                    validation_steps=len(VG),\n",
    "                    steps_per_epoch=len(DG),\n",
    "                    epochs=75,\n",
    "                    callbacks=[early_stopping, tb, reduce_on_plateau, checkpointer],\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating on test data...\")\n",
    "\n",
    "#from validation import sentence_metrics\n",
    "from generators import TestDataGenerator\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "TG = TestDataGenerator(test['data'], BATCH_SIZE, TM, True, True, True, True)\n",
    "\n",
    "actual, pred = list(), list()\n",
    "for batch in TG:\n",
    "    \n",
    "    X_data, Y_test = batch\n",
    "    Y_pred = model.predict_on_batch(X_data)\n",
    "    Y_pred, Y_test= np.argmax(Y_pred, axis = 2), np.argmax(Y_test, axis = 2)\n",
    "    \n",
    "    for i in range(Y_pred.shape[0]):\n",
    "        pred.append(TM.convert2tags(Y_pred[i, :]))\n",
    "        actual.append(TM.convert2tags(Y_test[i, :]))\n",
    "\n",
    "print(classification_report(actual, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
