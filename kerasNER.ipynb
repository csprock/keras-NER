{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from feature_utils import featureTransformer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from models.blstm_cnn_word_features_model import blstm_cnn_wd_ft_ner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CoNLL2003 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = json.load(open('C:/Users/csprock/Documents/Projects/NER/data/conll2003/en/train.json'))\n",
    "valid = json.load(open('C:/Users/csprock/Documents/Projects/NER/data/conll2003/en/valid.json'))\n",
    "test = json.load(open('C:/Users/csprock/Documents/Projects/NER/data/conll2003/en/test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary and tag lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS, TAGS = list(), list()\n",
    "for d in itertools.chain(train['data'], valid['data'], test['data']):\n",
    "    for w in d['sentence']:\n",
    "        if w.lower() not in WORDS: WORDS.append(w.lower())\n",
    "            \n",
    "    for t in d['tags']:\n",
    "        if t not in TAGS: TAGS.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sentence and word lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN_SENT = 50   # maximum sentence length\n",
    "MAX_LEN_WORD = 15   # maximum word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize feature transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_transformer = featureTransformer(WORDS, TAGS, tag_pad_value = 'O', zero_tag = 'O', \n",
    "                                      word_padding = 'post', word_truncating = 'post', \n",
    "                                      sent_truncating = 'post', sent_padding = 'post', \n",
    "                                      max_len_sent = MAX_LEN_SENT, max_len_word = MAX_LEN_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sent_train, X_char_train, X_word_ft_train, Y_train = feat_transformer.makeTensors(train['data'], sentences = True, characters = True, word_features = True, tags = True)\n",
    "X_sent_val, X_char_val, X_word_ft_val, Y_val = feat_transformer.makeTensors(valid['data'], sentences = True, characters = True, word_features = True, tags = True)\n",
    "X_sent_test, X_char_test, X_word_ft_test, Y_test = feat_transformer.makeTensors(test['data'], sentences = True, characters = True, word_features = True, tags = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and parse word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dir = 'drive/deeplearning/glove.6B.100d.txt'\n",
    "\n",
    "e = open(emb_dir, encoding = 'UTF-8')\n",
    "\n",
    "embeddings = dict()\n",
    "for line in e:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coef = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings[word] = coef\n",
    "    \n",
    "e.close()\n",
    "\n",
    "embedding_dim = (len(feat_transformer.word2idx), 100)\n",
    "E = np.zeros(embedding_dim)\n",
    "\n",
    "for i, w in enumerate(feat_transformer.word2idx):\n",
    "    emb_vec = embeddings.get(w)\n",
    "    if emb_vec is not None:\n",
    "        E[i,:] = emb_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = blstm_cnn_wd_ft_ner(max_len_sent = MAX_LEN_SENT, max_len_word = MAX_LEN_WORD, num_tags = len(feat_transformer.tag2idx), \n",
    "                            word_embedding_dims = embedding_dim, \n",
    "                            char_embedding_dims = (len(feat_transformer.char2idx), 25),\n",
    "                            word_feature_embedding_dims = (6,4))\n",
    "                            \n",
    "\n",
    "\n",
    "model.layers[7].set_weights([E])\n",
    "model.layers[7].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer = SGD(lr = 0.01), loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_acc', min_delta = 0.0001, patience = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x = {'word_input':X_sent_train, 'word_feature_input':X_word_ft_train, 'char_input':X_char_train}, \n",
    "          y = Y_train, \n",
    "          batch_size = 32, \n",
    "          validation_data = ({'word_input':X_sent_val, 'word_feature_input':X_word_ft_val, 'char_input':X_char_val}, Y_val), \n",
    "          epochs = 75, \n",
    "          callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(x = {'word_input':X_sent_test, 'word_feature_input':X_word_ft_test, 'char_input':X_char_test})\n",
    "\n",
    "# unroll predicted tensors\n",
    "Y_pred = np.argmax(Y_pred, axis = 2).reshape((Y_pred.shape[0]*Y_pred.shape[1],1))\n",
    "Y_test = np.argmax(Y_test, axis = 2).reshape((Y_pred.shape[0]*Y_pred.shape[1],1))\n",
    "\n",
    "# convert indices back to tags\n",
    "y_pred, y_true = [None]*Y_pred.shape[0], [None]*Y_test.shape[0]\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    y_pred[i] = feat_transformer.idx2tag[Y_pred[i][0]]\n",
    "    y_true[i] = feat_transformer.idx2tag[Y_test[i][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pint(classification_report(y_true = y_true, y_pred = y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
